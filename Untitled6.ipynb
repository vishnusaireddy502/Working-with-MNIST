{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qMsq7FNffpEA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "lr = 0.001\n",
        "num_epochs = 5\n"
      ],
      "metadata": {
        "id": "EIhmgOosgAyl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])"
      ],
      "metadata": {
        "id": "wqGBFKszgQAW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.MNIST(root = 'data', train = True, download = True, transform = transforms)\n",
        "test_dataset = datasets.MNIST(root = 'data', train = False, download = True, transform = transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
        "test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imLek7rrhHWw",
        "outputId": "c937ea62-2045-41ac-be4f-5c7fc980bcf1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 16660172.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 442802.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3563208.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2336628.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining Neural Network\n",
        "\n",
        "class neuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(neuralNet, self).__init__()\n",
        "        self.layer1 = nn.Linear(28*28, 512)\n",
        "        self.layer2 = nn.Linear(512, 256)\n",
        "        self.layer3 = nn.Linear(256, 10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.relu(self.layer1(x))\n",
        "        x = torch.relu(self.layer2(x))\n",
        "        x = self.layer3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "_HJORtJgi2Aq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = neuralNet()\n",
        "lossFn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = lr)"
      ],
      "metadata": {
        "id": "zEjWQywvklkZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "  for batch_idc, (data, label) in enumerate(train_loader):\n",
        "    outpus = model(data)\n",
        "    loss = lossFn(outpus, label)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (batch_idc + 1) % 100 == 0:\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idc+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu1byA2ekzW-",
        "outputId": "5fedf964-13d8-4fcd-aeab-af982298f0b2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/938], Loss: 2.2974\n",
            "Epoch [1/5], Step [200/938], Loss: 2.2709\n",
            "Epoch [1/5], Step [300/938], Loss: 2.2447\n",
            "Epoch [1/5], Step [400/938], Loss: 2.1980\n",
            "Epoch [1/5], Step [500/938], Loss: 2.1779\n",
            "Epoch [1/5], Step [600/938], Loss: 2.1393\n",
            "Epoch [1/5], Step [700/938], Loss: 2.1050\n",
            "Epoch [1/5], Step [800/938], Loss: 2.0548\n",
            "Epoch [1/5], Step [900/938], Loss: 2.0756\n",
            "Epoch [2/5], Step [100/938], Loss: 1.9314\n",
            "Epoch [2/5], Step [200/938], Loss: 1.9203\n",
            "Epoch [2/5], Step [300/938], Loss: 1.9645\n",
            "Epoch [2/5], Step [400/938], Loss: 1.8632\n",
            "Epoch [2/5], Step [500/938], Loss: 1.7837\n",
            "Epoch [2/5], Step [600/938], Loss: 1.7234\n",
            "Epoch [2/5], Step [700/938], Loss: 1.4980\n",
            "Epoch [2/5], Step [800/938], Loss: 1.5934\n",
            "Epoch [2/5], Step [900/938], Loss: 1.4894\n",
            "Epoch [3/5], Step [100/938], Loss: 1.4500\n",
            "Epoch [3/5], Step [200/938], Loss: 1.4719\n",
            "Epoch [3/5], Step [300/938], Loss: 1.2591\n",
            "Epoch [3/5], Step [400/938], Loss: 1.2921\n",
            "Epoch [3/5], Step [500/938], Loss: 1.1357\n",
            "Epoch [3/5], Step [600/938], Loss: 1.1061\n",
            "Epoch [3/5], Step [700/938], Loss: 1.0690\n",
            "Epoch [3/5], Step [800/938], Loss: 1.0865\n",
            "Epoch [3/5], Step [900/938], Loss: 1.1137\n",
            "Epoch [4/5], Step [100/938], Loss: 0.9769\n",
            "Epoch [4/5], Step [200/938], Loss: 1.0411\n",
            "Epoch [4/5], Step [300/938], Loss: 0.9685\n",
            "Epoch [4/5], Step [400/938], Loss: 0.8343\n",
            "Epoch [4/5], Step [500/938], Loss: 0.7379\n",
            "Epoch [4/5], Step [600/938], Loss: 0.7283\n",
            "Epoch [4/5], Step [700/938], Loss: 0.8773\n",
            "Epoch [4/5], Step [800/938], Loss: 0.8291\n",
            "Epoch [4/5], Step [900/938], Loss: 0.7857\n",
            "Epoch [5/5], Step [100/938], Loss: 0.8662\n",
            "Epoch [5/5], Step [200/938], Loss: 0.6530\n",
            "Epoch [5/5], Step [300/938], Loss: 0.7597\n",
            "Epoch [5/5], Step [400/938], Loss: 0.6856\n",
            "Epoch [5/5], Step [500/938], Loss: 0.9107\n",
            "Epoch [5/5], Step [600/938], Loss: 0.5578\n",
            "Epoch [5/5], Step [700/938], Loss: 0.6323\n",
            "Epoch [5/5], Step [800/938], Loss: 0.6433\n",
            "Epoch [5/5], Step [900/938], Loss: 0.5973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "with torch.inference_mode():\n",
        "  for data, label in test_loader:\n",
        "    outputs = model(data)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    accuracy = (predicted == label).sum()\n",
        "    total_correct += accuracy\n",
        "    total_samples += len(label)\n",
        "  print(f'Accuracy: {(total_correct/total_samples)*100:.4f},  fraction = {total_correct}/{total_samples}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir2wK9_9nVB1",
        "outputId": "0534924a-4526-4484-aa9e-b5f9e4e3cfe5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 85.9000,  fraction = 8590/10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bpo_PS9MqQwm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}